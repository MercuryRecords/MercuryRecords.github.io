---
layout: post
title: OS | Lab 5 文档
date: 2024-08-01 17:00:00 +0800
categories: [OS]
description: 操作系统 Lab 5 文档
keywords: OS
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

# Lab 5

## 一、思考题

### Thinking 5.1

> 如果通过 `kseg0` 读写设备，那么对于设备的写入会缓存到 `Cache` 中。这是一种**错误**的行为，在实际编写代码的时候这么做会引发不可预知的问题。请思考：这么做这会引发什么问题？对于不同种类的设备（如我们提到的串口设备和 `IDE` 磁盘）的操作会有差异吗？可以从缓存的性质和缓存更新的策略来考虑。 

- **引发的问题：**当 CPU 向外部设备写入数据时，如果数据仅存在于 `Cache` 中而未及时同步到设备，那么当设备自身进行数据更新时， `Cache` 中的数据将变得过时。这种情况下，缓存数据只有在缓存块被新数据替换时才会被写入到设备，可能导致数据不一致的问题。

- **差异：**串口设备较 IDE 磁盘读写更加频繁，使用 `kseg0` 读写时更容易引发数据不一致的问题。

### Thinking 5.2

> 查找代码中的相关定义，试回答一个磁盘块中最多能存储多少个文件控制块？一个目录下最多能有多少个文件？我们的文件系统支持的单个文件最大为多大？ 

```c
#define FILE_STRUCT_SIZE 256
#define BLOCK_SIZE PAGE_SIZE // 4096
#define NINDIRECT (BLOCK_SIZE / 4) // 1024
```

- **一个磁盘块中最大文件控制块数量：** 4096 / 256 = 16 个
- **一个目录下最大文件数量：** 由 `NINDIRECT` 定义知最大文件数量为 1024 个
- **单个文件最大大小：** 每个文件最多指向 1024 个磁盘块，每个磁盘块的大小为 4096 = 4 KB，则所求为 1024 * 4 KB = 4 MB

> 对于普通的文件，其指向的磁盘块存储着**文件内容**，而对于目录文件来说，其指向的磁盘块存储着**该目录下各个文件对应的文件控制块**。


### Thinking 5.3

> 请思考，在满足磁盘块缓存的设计的前提下，我们实验使用的内核支持的最大磁盘大小是多少？ 

```c
#define DISKMAP 0x10000000
#define DISKMAX 0x40000000
```

块缓存对应的内存空间为 `[DISKMAP, DISKMAP + DISKMAX)`，故实验用内核支持的最大磁盘大小为 0x40000000 bytes = 1GB

### Thinking 5.4

> 在本实验中，`fs/serv.h`、`user/include/fs.h` 等文件中出现了许多宏定义，试列举你认为较为重要的宏定义，同时进行解释，并描述其主要应用之处。 

见注释

```c
// serv.h
#define PTE_DIRTY 0x0004 // 虚拟地址脏位，注意区分 va_is_dirty 和 block_is_dirty （后调用前）
#define SECT_SIZE 512 // 磁盘扇区大小，磁盘的最小组成单元
#define SECT2BLK (BLOCK_SIZE / SECT_SIZE) // 这么多个扇区构成一个磁盘块，用于读写磁盘块时辨析一个磁盘块到扇区的数目
/* 涉及到 ide_write、ide_read，记录一下使用：

void test_ide_write(u_int diskno) {
	int data[4096]; // [1]
	int i;
	for (i = 0; i < 4096; i++) {
		data[i] = i ^ diskno;
	}
	ide_write(diskno, 4096, data, 32);
}

对于参数 32 的计算，由[1]得出有 4 * 4096 个字节的数据，每个 sector 有 512 字节，4 * 4096 / 512 = 32。所以其实可以有如下宏定义：

#define SECT2DATA_SIZE(n) ((n) / SECT_SIZE)

*/
#define DISKMAP 0x10000000 // 虚存地址空间中磁盘块起始地址
#define DISKMAX 0x40000000 // 作用在 5.3 里提过了，可以用于越界检查
```

```c
// fs.h
#define BLOCK_SIZE PAGE_SIZE // 磁盘块大小
#define BLOCK_SIZE_BIT (BLOCK_SIZE * 8) // 按位而不是字节计数的磁盘块大小
#define MAXNAMELEN 128 // 文件名最大长度
#define MAXPATHLEN 1024 // 路径最大长度


#define NDIRECT 10 // 磁盘块直接指针数
#define NINDIRECT (BLOCK_SIZE / 4) // 磁盘块简介指针数，为了方便使用弃用了前 10 个

/* 用例见 tools/fsformat.c: create_file */

#define MAXFILESIZE (NINDIRECT * BLOCK_SIZE) // 磁盘中最大文件数量？？没太搞懂是怎么算出来的
#define FILE_STRUCT_SIZE 256 // 文件控制块变量大小

// 文件控制块的定义
struct File {
	char f_name[MAXNAMELEN];
	uint32_t f_size;
	uint32_t f_type;
	uint32_t f_direct[NDIRECT];
	uint32_t f_indirect;

	struct File *f_dir; // the pointer to the dir where this file is in, valid only in memory.
	char f_pad[FILE_STRUCT_SIZE - MAXNAMELEN - (3 + NDIRECT) * 4 - sizeof(void *)]; // 填充用
} __attribute__((aligned(4), packed));

#define FILE2BLK (BLOCK_SIZE / sizeof(struct File)) // 用于在磁盘块中遍历文件控制块

// File types
#define FTYPE_REG 0 // Regular file
#define FTYPE_DIR 1 // Directory

#define FS_MAGIC 0x68286097 // Everyone's favorite OS class

// 超级块的定义

struct Super {
	uint32_t s_magic;   // 魔数，grep -r s_magic 可以发现 read_super 函数中确实有对文件系统魔数的检查
	uint32_t s_nblocks; // Total number of blocks on disk
	struct File s_root; // Root directory node
};

```

### Thinking 5.5

> 在 Lab4“系统调用与 fork”的实验中我们实现了极为重要的 fork 函数。那么 fork 前后的父子进程是否会共享文件描述符和定位指针呢？请在完成上述练习的基础上编写一个程序进行验证。 

编写如下：

```c

#include <lib.h>

int main() {
	
	int r;
	int fdnum;
	char buf[512];
	int n;

	if ((r = open("/newmotd", O_RDWR) < 0)) {
		user_panic("open failed with returned value: %d", r);
	}

	fdnum = r;
	if (fork() == 0) {
		n = read(fdnum, buf, 5);
		debugf("child's is %s\n", buf);
	} else {
		n = read(fdnum, buf, 5);
		debugf("parent's is %s\n", buf);
	}
	return 0;
}

```

运行结果：
```c
parent's is This 
child's is is a 
```

可以看出父子进程回共享文件描述符和定位指针。

### Thinking 5.6

> 请解释 `File`, `Fd`, `Filefd` 结构体及其各个域的作用。比如各个结构体会在哪些过程中被使用，是否对应磁盘上的物理实体还是单纯的内存数据等。说明形式自定，要求简洁明了，可大致勾勒出文件系统数据结构与物理实体的对应关系与设计框架。

```c

struct File {
	char f_name[MAXNAMELEN]; // 文件名，最大长度为 MAXNAMELEN
	uint32_t f_size; // 对应文件大小
	uint32_t f_type; // 对应文件类型，分为普通文件和目录
	uint32_t f_direct[NDIRECT]; // 文件直接指针，直接指向数据块对应位置
	uint32_t f_indirect; // 文件间接指针，指向文件内容的磁盘块

	struct File *f_dir; // 指向文件所在目录的指针，只有控制块已经在内存中时才有效
	char f_pad[FILE_STRUCT_SIZE - MAXNAMELEN - (3 + NDIRECT) * 4 - sizeof(void *)]; // 填充用
} __attribute__((aligned(4), packed)); // 强制对齐

struct Fd {
	u_int fd_dev_id; // 外设id
	u_int fd_offset; // 读写的偏移量，利用其的函数包括 write()、read()、seek()，由这类功能的函数使用和维护 
	u_int fd_omode; // 文件打开的方式，包括 O_RDONLY O_WRONLY O_RDWR
};

struct Filefd {
	struct Fd f_fd; // 对应文件描述符结构体
	u_int f_fileid; // 文件id
	struct File f_file; // 对应文件控制块
};

```

Filefd 以及 fd 中的指向的文件控制块 File 中记录的磁盘指针对应物理实体

### Thinking 5.7

> 下图（文件系统服务时序图）中有多种不同形式的箭头，请解释这些不同箭头的差别，并思考我们的操作系统是如何实现对应类型的进程间通信的。

在操作系统的初始化过程中，`init()` 函数负责启动并发送 `ENV_CREATE` 的异步消息，包含 `user_env` 和 `fs_serv`。一旦 `init()` 发出这些创建消息，它会立即返回并继续执行其它初始化步骤，与此同时，`fs` 进程和 `user` 进程将独立进行各自的初始化任务。

对于 `fs` 进程，它首先会执行 `serv_init()` 和 `fs_init()` 函数来完成初始化。随后，`fs` 进程会进入 `serv()` 函数，在这里它会被 `ipc_receive()` 调用阻塞，进入 `ENV_NOT_RUNNABLE` 状态，直到它接收到来自 `user` 进程的 `ipc_send(fsreq)` 消息，这个动作会唤醒 `fs` 进程。

另一方面，`user` 进程通过 `ipc_send(fsreq)` 向 `fs` 进程发送一个同步消息，请求服务。发送消息后，`user` 进程自身也会进入 `ENV_NOT_RUNNABLE` 状态，等待 `fs` 进程完成服务处理。当 `fs` 进程处理完请求，并通过 `ipc_send(dst_va)` 发送回应后，`user` 进程被唤醒并继续执行。此后，`fs` 进程再次进入阻塞状态，等待下一次 `user` 进程的唤醒。

## 二、难点分析

文件系统屏蔽了访问外存数据的复杂性，但是在实现上也相当复杂，主要是有很多的代码文件，分布在三个主要目录：`tools`、`fs`、`user/lib`。然后继承上一个 lab 的理解，才能搞懂内核态的磁盘驱动和用户态读写磁盘等内容。真要说难点，基本都分散在思考题里面了，文件系统我感觉不算难，主要是体系比较庞大，需要慢慢消化。

## 三、实验体会

### exam

这一次要补充的代码也太多了，bug修复包括忘记加 `file_dirty` 、`serve_copy` 调用时应该写 `directory_copy` 而不是 `copy_directory_contents` 、手滑写成 `FSREQ_OPEN` 而不是 `FSREQ_COPY`、对字符串应该调用 `strcpy` 而不能直接赋值，然后对 `file_dirty` 理解失误填错参数、`dir_alloc_file` 写错目录参数。

### extra

写了一堆代码之后没时间 debug 了。

这就是失眠只睡了四个半小时之后的线上限时测验，不过总算结束了。